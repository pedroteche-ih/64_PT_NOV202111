{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine = pd.read_csv(\"data/wine-clustering.csv\")\n",
    "tb_wine.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis Continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalers\n",
    "\n",
    "Scalers são transformações que buscam tornar as variáveis em um dataset comparáveis entre si. O mais comum de se utilizar é o StandardScaler, que utilizamos extensamente ao longo das últimas semanas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler - Normalização\n",
    "\n",
    "O objeto `StandardScaler` subtrai a média e divide a variável pelo desvio padrão. Essa transformação é utilizada para remover a unidade (metros, litros, número de pessoas...) da variável: a nova variável calculada tem como unidade **Desvios Padrões**, *ela será 0 na média e 1 em média + 1 desvio padrão*.\n",
    "\n",
    "**QUANDO UTILIZAR:** Toda vez que formos utilizar features em um modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=tb_wine, x=\"Alcohol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos o objeto\n",
    "alcohol_scaler = StandardScaler()\n",
    "# Utilizamos o método fit_transform para calcular a transformação\n",
    "# e retornar a variável transformada de uma vez.\n",
    "tb_wine[\"alcohol_sc\"] = alcohol_scaler.fit_transform(tb_wine[[\"Alcohol\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "sns.kdeplot(data=tb_wine, x=\"Alcohol\", ax=ax[0])\n",
    "sns.kdeplot(data=tb_wine, x=\"alcohol_sc\", ax=ax[1])\n",
    "fig.suptitle(\"Visualizando o Efeito do StandardScaler\\n sobre a variável Alcohol\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o método `.fit` seguido do método `.transform`. O método `.fit` (assim como o `.fit_transform`) pode normalizar **muitas variáveis ao mesmo tempo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine = tb_wine.drop(\"alcohol_sc\", axis=1)\n",
    "wine_scaler = StandardScaler()\n",
    "wine_scaler.fit(tb_wine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os métodos da biblioteca `sklearn` retornam **arrays do numpy**, não dataframes. Se quisermos guardar o resultado de transformações em dataframes (para visualização por exemplo) precisamos faze-lo explicitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine_sca = pd.DataFrame(wine_scaler.transform(tb_wine), columns=tb_wine.columns)\n",
    "tb_wine_sca.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tb_wine_sca, vars=[\"Alcohol\", \"Malic_Acid\", \"Ash\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver no gráfico acima que as variáveis normalizadas tem média 0 e que estão variando entre aprox. -3 e 3 desvios padrões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PowerTransformer\n",
    "\n",
    "As transformações de potência buscam tornar uma variável mais **normal** (no sentido da distribuição probabilística). Ela é uma extensão da transformação logarítmica. O objeto `PowerTransformer` realiza essa transformação, retornando uma variável transformada e normalizada (no sentido do `StandardScaler`).\n",
    "\n",
    "**QUANDO UTILIZAR:** Sempre que pensarmos em utilizar o logaritmo de uma variável (resposta ou feature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar o efeito do `PowerTransformer` para a variável `Malic_Acid` com a variável original e sua normalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malic_ptran = PowerTransformer()\n",
    "malic_scaler = StandardScaler()\n",
    "tb_wine[\"malic_acid_pt\"] = malic_ptran.fit_transform(\n",
    "    tb_wine[[\"Malic_Acid\"]]\n",
    ")  # NOTEM O USO DE COLCHETES DUPLOS!!!\n",
    "tb_wine[\"malic_acid_sc\"] = malic_scaler.fit_transform(\n",
    "    tb_wine[[\"Malic_Acid\"]]\n",
    ")  # NOTEM O USO DE COLCHETES DUPLOS!!!\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "sns.kdeplot(data=tb_wine, x=\"Malic_Acid\", ax=ax[0])\n",
    "\n",
    "sns.kdeplot(data=tb_wine, x=\"malic_acid_sc\", ax=ax[1])\n",
    "sns.kdeplot(data=tb_wine, x=\"malic_acid_pt\", ax=ax[2])\n",
    "ax[0].set_title(\"Variável Original\")\n",
    "ax[1].set_title(\"StandardScaler\")\n",
    "ax[2].set_title(\"PowerTransform\")\n",
    "ax[0].set_xlim(-7, 7)\n",
    "ax[1].set_xlim(-7, 7)\n",
    "ax[2].set_xlim(-7, 7)\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[2].set_xlabel(\"\")\n",
    "\n",
    "fig.suptitle(\"Visualizando o Efeito do PowerTransformer\\n sobre a variável Malic_Acid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver no gráfico acima o PowerTransformer torna a variável mais **simétrica**: os valores estão mais bem distribuidos ao redor da média. Podemos utilizar o método `.inverse_transform` para recuperar a variável original (esse método também existe para o `StandardScaler`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine[\"malic_acid_it\"] = malic_ptran.inverse_transform(tb_wine[[\"malic_acid_pt\"]])\n",
    "sns.kdeplot(data=tb_wine, x=\"malic_acid_it\")\n",
    "tb_wine = tb_wine.drop([\"malic_acid_sc\", \"malic_acid_pt\", \"malic_acid_it\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Features\n",
    "Além da normalização de features podemos utilizar alguns métodos do sub-módulo `sklearn.preprocessing` para criar novas features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures\n",
    "\n",
    "Muitas relações entre variáveis não são lineares - algumas destas são polinomias (como a Lei da Gravidade, que é uma relação quadrática). Podemos representar essas relações em modelos lineares utilizando uma transformação polinomial.\n",
    "\n",
    "**QUANDO UTILIZAR** Sempre que quisermos representar efeitos não-lineares em um modelo linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar features quadráticas (degree = 2) para a variável \n",
    "# Alcohol, possibilitando modelar impactos não lineares\n",
    "# desta variável sobre outras variáveis.\n",
    "alcohol_poly = PolynomialFeatures(degree = 2)\n",
    "alcohol_poly.fit(tb_wine[['Alcohol']])\n",
    "alcohol_poly.transform(tb_wine[['Alcohol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos guardar o resultado dessa transformação em um DataFrame precisamos faze-lo explicitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_alcohol_quad = pd.DataFrame(alcohol_poly.transform(tb_wine[['Alcohol']]), columns = alcohol_poly.get_feature_names_out())\n",
    "tb_alcohol_quad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spline Transformer\n",
    "\n",
    "Muitas vezes a escala de uma variável é determinante no impacto desta sobre outra variável: como vimos no exemplo de seguros de saúde, o preço era impactado pelo BMI apenas quando este passava de 30. Em modelos mais complexos não conseguimos visualizar tão claramente essas relações mas podemos utilizar **b-splines** para tentar representa-los.\n",
    "\n",
    "**QUANDO UTILIZAR:** Quando suspeitarmos que o efeito de uma variável é **local**, por exemplo, considerando o feature `Tempo` os lockdowns devido ao COVID-19 foram efeitos locais do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os parâmetros n_knots e degree determinam quantos splines \n",
    "# criaremos para uma variavel: n_knots + degree - 1\n",
    "alcohol_spline = SplineTransformer(n_knots = 5, degree = 5)\n",
    "alcohol_spline.fit(tb_wine[['Alcohol']])\n",
    "alcohol_spline.transform(tb_wine[['Alcohol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos utilizar o resultado em um DataFrame, precisamos construí-lo explicitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_sp_alc = pd.DataFrame(alcohol_spline.transform(tb_wine[['Alcohol']]), columns = alcohol_spline.get_feature_names_out())\n",
    "tb_sp_alc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis Categóricas\n",
    "\n",
    "As variáveis categóricas precisam ser transformadas em variáveis numéricas antes de podermos utiliza-las em modelos da sklearn. Podemos adotar duas estratégias para esta transformação: converte-las em variáveis ordinais ou em variáveis dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Ordinais\n",
    "Variáveis ordinais são variáveis categóricas que tem uma escala, por exemplo uma variável com os níveis *Muito Infeliz, Infeliz, Neutro, Feliz, Muito Feliz*. Podemos converter esse tipo de variável em uma variável numérica assumindo que os degraus entre cada nível das variáveis é igual entre si.\n",
    "\n",
    "**QUANDO UTILIZAR:** Quando nossa variável categórica for ordenável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine[\"alcohol_classif\"] = pd.qcut(tb_wine[\"Alcohol\"], 3, labels=[\"A\", \"B\", \"C\"])\n",
    "tb_wine[\"alcohol_classif\"] = tb_wine[\"alcohol_classif\"].astype(str)\n",
    "tb_wine[\"alcohol_classif\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_alcohol = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
    "tb_wine[\"alcohol_classif_num\"] = tb_wine[\"alcohol_classif\"].map(dict_alcohol)\n",
    "tb_wine[\"alcohol_classif_num\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "Podemos transformar utilizar o OneHotEncoder para criar variáveis dummies a partir de uma variável categórica, desta forma converteremos cada nível da variável categórica em uma nova variável binária.\n",
    "\n",
    "**QUANDO UTILIZAR:** Quando nossa variável categórica não tiver um ordenamento regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos inicializar o OneHotEncoder com 2 hiperparâmetros.\n",
    "# drop = 'first' criará n-1 variáveis dummies, onde n é o número de níveis.\n",
    "# sparse = False permitirá a transformação do array resultante em um DataFrame.\n",
    "ohe_fit = OneHotEncoder(drop=\"first\", sparse=False)\n",
    "ohe_fit.fit(tb_wine[['alcohol_classif']])\n",
    "ohe_fit.transform(tb_wine[['alcohol_classif']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos utilizar isto em um DataFrame, precisamos cria-lo explicitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_ac = pd.DataFrame(ohe_fit.transform(tb_wine[['alcohol_classif']]), columns = ohe_fit.get_feature_names_out())\n",
    "tb_ac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine = tb_wine.drop(['alcohol_classif', 'alcohol_classif_num'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Não-Supervisionados\n",
    "\n",
    "**Aulas** \n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220317 PCA.ipynb`\n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220324 Unsupervised Learning I.ipynb`\n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220324 Unsupervised Learning II.ipynb`\n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220324 Unsupervised Learning III.ipynb`\n",
    "\n",
    "\n",
    "**Case**\n",
    "* **Padrões de Movimento de Urubus (Aglomerativo):** `cases/Padrões de Movimento - Urubu.ipynb` \n",
    "* **Padrões de Movimento de Zebras (DBSCAN/HDBSCAN):** `cases/Padrões de Movimento - Zebras.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "PCA é um método de decomposição que transforma um conjunto de variáveis em outro conjunto de variáveis que não são correlatas entre si.\n",
    "\n",
    "**QUANDO UTILIZAR:**\n",
    "* **EDA:** para entender se nosso feature set tem muitas colinearidades, analisando a proporção entre o número de componentes, a variação explicada e o número de variáveis completas (scree-plot).\n",
    "* **MODELAGEM:** para tratar de colinearidade entre variáveis na entrada de um modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo número de componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine = PCA()\n",
    "pca_wine.fit(wine_scaler.transform(tb_wine))  # SEMPRE FAZER PCA COM DADOS NORMALIZADOS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree plot representa quantos % da variância total das variáveis cada componente representa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_wine.explained_variance_ratio_);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca_wine.explained_variance_ratio_));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método do cotovelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_fit = KneeLocator(\n",
    "    range(\n",
    "        len(pca_wine.explained_variance_ratio_)\n",
    "    ),  # vetor com número dos componentes de 0 à 12\n",
    "    np.cumsum(\n",
    "        pca_wine.explained_variance_ratio_\n",
    "    ),  # soma acumulada da variância explicada por cada componente\n",
    ")\n",
    "print(f\"Método do cotovelo sugere: {knee_fit.knee} Componentes\")\n",
    "knee_fit.plot_knee_normalized()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos utilizar o número de componentes calculado no atributo `knee_fit.knee` para calcular nosso PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine = PCA(n_components=knee_fit.knee)\n",
    "ar_pca_wine = pca_wine.fit_transform(\n",
    "    wine_scaler.transform(tb_wine)\n",
    ")  # SEMPRE NORMALIZAR DADOS ANTES DE PCA\n",
    "tb_pca_wine = pd.DataFrame(\n",
    "    ar_pca_wine, columns=[\"PC\" + str(i) for i in range(knee_fit.knee)]\n",
    ")\n",
    "sns.pairplot(tb_pca_wine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "KMeans é uma técnica de clusterização simples que utiliza um método iterativo para encontrar um número pré-definido de clusters.\n",
    "\n",
    "**QUANDO UTILIZAR:** é uma técnica rápida (computacionalmente) que nos permite criar uma nova variável categórica através da qual podemos explorar nossos dados. \n",
    "\n",
    "> K-means is the simplest. To implement and to run. All you need to do is choose \"k\" and run it a number of times. \n",
    "> \n",
    "> Most more clever algorithms (in particular the good ones) are much harder to implement efficiently (you'll see factors of 100x in runtime differences) and have much more parameters to set. \n",
    "> \n",
    "> Plus, most people don't need quality clusters. They actually are happy with anything remotely working for them. Plus, they don't really know what to do when they had more complex clusters. K-means, which models clusters using the simplest model ever - a centroid - is exactly what they need: massive data reduction to centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo número de clusters - Método do Cotovelo\n",
    "Vamos utilizar a inércia e a silhueta, duas medidas de qualidade de clusters para encontrar um K **que faça sentido**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_list = []\n",
    "sil_list = []\n",
    "tb_sca_wine = wine_scaler.transform(tb_wine)\n",
    "for k in range(2, 16):\n",
    "    kmeans_fit = KMeans(n_clusters = k)\n",
    "    kmeans_fit.fit(tb_sca_wine) # SEMPRE NORMALIZAR ANTES DE KMEANS\n",
    "    inertia_list.append(kmeans_fit.inertia_)\n",
    "    sil_list.append(silhouette_score(tb_sca_wine, kmeans_fit.labels_))\n",
    "tb_score = pd.DataFrame({'k' : range(2, 16), 'inertia' : inertia_list, 'silhouette' : sil_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "sns.lineplot(data = tb_score, x = 'k', y = 'inertia', ax = ax[0])\n",
    "sns.lineplot(data = tb_score, x = 'k', y = 'silhouette', ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_fit = KneeLocator(\n",
    "    tb_score['k'],\n",
    "    np.cumsum(tb_score['inertia'])\n",
    ")\n",
    "print(f\"Método do cotovelo sugere: {knee_fit.knee} Componentes\")\n",
    "knee_fit.plot_knee_normalized()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação automatica sugere 8 clusters, mas existe bastante evidencia para escolhermos 3 clusters:\n",
    "* Maior queda de inércia entre k = 2 e k = 3\n",
    "* Melhor silhueta entre todos os k's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_fit = KMeans(n_clusters=3)\n",
    "kmeans_fit.fit(wine_scaler.transform(tb_wine))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_pca_wine[\"km_clu\"] = kmeans_fit.labels_\n",
    "# ou tb_wine['km_clu'] = kmeans_fit.predict(tb_sca_wine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando PCA para visualizar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tb_pca_wine, hue=\"km_clu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aglomerativo (Hierárquico)\n",
    "Enquanto `KMeans` é um algoritmo que opera de cima para baixo, os algoritmos hierárquicos operam de baixo para cima: a partir de uma função de distância (`affinity` ou `metric`), junto cada ponto ao seu vizinho mais próximo. Com esses clusters, a partir de uma função de `linkage`, junta iterativemente cada grupo ao grupo mais próximo até juntar todos os pontos em um único grupo.\n",
    "\n",
    "Essa estrutura hierárquica pode ser visualizada através do dendograma.\n",
    "\n",
    "**QUANDO UTILIZAR:**\n",
    "* **EDA:** quando queremos explorar a estrutura de proximidade entre nossos pontos sem reduzi-los às medidas de inércia e silhueta.\n",
    "* **MODELAGEM:** \n",
    "    * Quando precisamos entender uma clusterização a partir de um número pré-definido de clusters (por exemplo, a área de vendas nos informa que temos 3 tipos de clientes - queremos investigar como essa impressão qualitativa dialoga com os dados). \n",
    "    * Quando nossos grupos não tem uma separação clara.\n",
    "\n",
    "**Padrões de voos de urubus:** `cases/Padrões de Movimento - Urubu.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo dendograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram_ = dendrogram(linkage(wine_scaler.transform(tb_wine), method=\"ward\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Cortando' dendograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = AgglomerativeClustering(\n",
    "    n_clusters=3, affinity=\"euclidean\", linkage=\"ward\"\n",
    ")\n",
    "hierarchical.fit(wine_scaler.transform(tb_wine))\n",
    "tb_pca_wine[\"ward_clu\"] = [str(x) for x in hierarchical.labels_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando usando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tb_pca_wine, hue=\"ward_clu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "O algoritmo DBSCAN agrupa pontos que se encontram em uma mesma região densa. Pontos fora de regiões densas são marcados como outliers. É uma técnica avançada que possibilita a determinação do número de clusters a partir da propria distribuição desses pontos em nossos features\n",
    "\n",
    "**QUANDO UTILIZAR:** \n",
    "* Quando não temos nenhuma idéia sobre a quantidade de clusters. \n",
    "* Quando queremos estimar muitos clusters (mais que 10 por exemplo). \n",
    "    * Funciona muito bem para dados geográficos (onde os features são a latitude e a longitude) para encontrar aglomerações de pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando hiperparâmetro `eps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dbscan_eps(data, clu_vars, neigh):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    scale_cludata = StandardScaler().fit_transform(data[clu_vars])\n",
    "    neighbors = neigh\n",
    "    nbrs = NearestNeighbors(n_neighbors=neighbors)\n",
    "    nbrs.fit(scale_cludata)\n",
    "    distances, indices = nbrs.kneighbors(scale_cludata)\n",
    "    distance_desc = sorted(distances[:, -1], reverse=True)\n",
    "    kneedle = KneeLocator(\n",
    "        range(1, len(distance_desc) + 1),\n",
    "        distance_desc,\n",
    "        S=5,\n",
    "        curve=\"convex\",\n",
    "        direction=\"decreasing\",\n",
    "    )\n",
    "    l_bound = int(np.where(np.array(distance_desc) == kneedle.knee_y)[0] * 0.1)\n",
    "    u_bound = int(np.where(np.array(distance_desc) == kneedle.knee_y)[0] * 1.9)\n",
    "\n",
    "    hist, bins = np.histogram(distances[:, -1], bins=20)\n",
    "    logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].hist(distances[:, -1], bins=logbins)\n",
    "    ax[0].set_xscale(\"log\")\n",
    "    ax[0].set_title(\"Distribuição de Distância\")\n",
    "    ax[0].grid(which=\"both\", linestyle=\"--\")\n",
    "    ax[1].plot(distance_desc[l_bound : u_bound + 1])\n",
    "    ax[1].axvline(\n",
    "        np.where(np.array(distance_desc[l_bound : u_bound + 1]) == kneedle.knee_y)\n",
    "    )\n",
    "    ax[1].set_title(\"\")\n",
    "\n",
    "    kneedle.plot_knee_normalized(figsize=(10, 5))\n",
    "    fig.suptitle(\n",
    "        f\"DBSCAN Eps Optimization\\nKnee found at: {round(kneedle.knee_y, 4)} w/ {neigh}-NN\",\n",
    "        y=1.05,\n",
    "    )\n",
    "    return round(kneedle.knee_y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_opt = find_dbscan_eps(tb_wine, tb_wine.columns, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_fit = DBSCAN(eps = eps_opt, min_samples = 3)\n",
    "db_fit.fit(wine_scaler.transform(tb_wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando resultados com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_pca_wine['db_clu'] = [str(x) for x in db_fit.labels_]\n",
    "sns.pairplot(data=tb_pca_wine, hue=\"db_clu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine['classif_wine'] = [str(x) for x in hierarchical.labels_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Supervisionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação\n",
    "\n",
    "**Aulas**\n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220329 Regressão Logistica.ipynb`\n",
    "* `aulas/64 DA PT NOV-2021 Aula 20220331 Métodos de Classificação.ipynb`\n",
    "\n",
    "**Cases**\n",
    "* `cases/64 DA PT NOV-2021 Case Hotel.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística\n",
    "Modelo de classificação mais simples- não tem hiperparâmetros e nos ajuda a construir um modelo compreensível, que nos fornece um bom baseline de erro de classificação.\n",
    "\n",
    "**QUANDO UTILIZAR** \n",
    "* Quando queremos um resultado interpretável. \n",
    "* Quando estamos mais interessados na estimativa de probabilidade do evento do que na classificação em si.\n",
    "* Como baseline de erro para modelos mais complexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhendo as variáveis do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = ['Alcohol', 'Malic_Acid']\n",
    "y_var = 'classif_wine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando e fitando a regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scaler = StandardScaler().fit(tb_wine[X_var])\n",
    "log_fit = LogisticRegression()\n",
    "log_fit.fit(log_scaler.transform(tb_wine[X_var]), tb_wine[y_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardando as previsões na tabela original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_wine['pred_log'] = log_fit.predict(log_scaler.transform(tb_wine[X_var]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando a matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(tb_wine['classif_wine'], tb_wine['pred_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando os scores de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precisão: {precision_score(tb_wine['classif_wine'], tb_wine['pred_log'], average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(tb_wine['classif_wine'], tb_wine['pred_log'], average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(tb_wine['classif_wine'], tb_wine['pred_log'], average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvores de Decisão\n",
    "\n",
    "As árvores de decisão são um dos métodos mais tradicionais de ML, no entanto hoje em dia raramente as utilizamos: árvores com bom poder preditivo não são interpretáveis, e as intepretáveis muitas vezes tem erro pior que uma regressão logística. No entanto, como elas compõe a base dos modelos de ensemble, é importante conhece-las.\n",
    "\n",
    "**QUANDO UTILIZAR** Em alguns problemas, uma árvore de decisão simples tem performance melhor que uma regressão logística e é de fácil interpretação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando em test e train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tb_wine[X_var], tb_wine[y_var], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_scaler = StandardScaler()\n",
    "tree_scaler.fit(X_train)\n",
    "tree_fit = DecisionTreeClassifier()\n",
    "tree_fit.fit(tree_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = tree_fit.predict(tree_scaler.transform(X_test))\n",
    "print(f\"Precisão: {precision_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, test_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth' : [int(x) for x in np.linspace(2, 50, 5)] + [None],\n",
    "    'min_samples_split' : [int(x) for x in np.linspace(2, 40, 5)],\n",
    "    'min_samples_leaf' : [int(x) for x in np.linspace(1, 20, 5)]\n",
    "}\n",
    "tree_fit = DecisionTreeClassifier()\n",
    "clf_fit = GridSearchCV(tree_fit, param_grid)\n",
    "clf_fit.fit(tree_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf_fit.predict(tree_scaler.transform(X_test))\n",
    "print(f\"Precisão: {precision_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, test_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de Ensemble\n",
    "\n",
    "Os métodos de ensemble utilizam diversas árvores de decisão fracas para construir um estimador robusto, com risco menor de overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "O modelo de bagging mais tradicional é a floresta aleatória: construímo `n_estimators` árvores de decisão com amostrar dos dados originais. Dessa forma cada árvore tem um risco menor de overfitting. No fim utilizamos a previsão de todas em um sistema de votação.\n",
    "\n",
    "**QUANDO UTILIZAR:** Junto com boosting é um dos métodos de melhor performance preditiva (em geral). Sempre que não nos interessa ter um modelo interpretável - onde o único requisito é a precisão da estimativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = RandomForestClassifier()\n",
    "rf_fit.fit(tree_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rf_fit.predict(tree_scaler.transform(X_test))\n",
    "print(f\"Precisão: {precision_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, test_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth' : [int(x) for x in np.linspace(1, 20, 5)] + [None],\n",
    "    'n_estimators' : [int(x) for x in np.linspace(100, 2000, 10)]\n",
    "}\n",
    "rf_fit = RandomForestClassifier()\n",
    "clf_fit = GridSearchCV(rf_fit, param_grid)\n",
    "clf_fit.fit(tree_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf_fit.predict(tree_scaler.transform(X_test))\n",
    "print(f\"Precisão: {precision_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, test_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "\n",
    "Representam a tecnologia de ponta preditiva através dos algoritmos CATBOOST, XGBOOST e LightGBM (junto aos modelos de deep learning).\n",
    "\n",
    "**QUANDO UTILIZAR:** Melhores métodos para performance preditiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = cat.CatBoostClassifier(iterations = 20000, depth = 3, auto_class_weights= \"Balanced\", od_type = \"Iter\", od_wait = 500)\n",
    "cat_fit.fit(tree_scaler.transform(X_train), y_train, eval_set = (tree_scaler.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = cat_fit.predict(tree_scaler.transform(X_test))\n",
    "print(f\"Precisão: {precision_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, test_pred, average = 'weighted')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, test_pred, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var = ['Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium',\n",
    "       'Total_Phenols', 'Flavanoids', 'Nonflavanoid_Phenols',\n",
    "       'Proanthocyanins', 'Color_Intensity', 'Hue', 'OD280', 'Proline']\n",
    "y_var = 'Alcohol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tb_wine[X_var], tb_wine[y_var], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scaler = StandardScaler()\n",
    "reg_scaler.fit(X_train)\n",
    "lin_fit = LinearRegression()\n",
    "lin_fit.fit(reg_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = lin_fit.predict(reg_scaler.transform(X_test))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lin_fit.coef_, index = X_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressões Regularizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scaler = StandardScaler()\n",
    "reg_scaler.fit(X_train)\n",
    "las_fit = LassoCV(cv = 5)\n",
    "las_fit.fit(reg_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = las_fit.predict(reg_scaler.transform(X_test))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(las_fit.coef_, index = X_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scaler = StandardScaler()\n",
    "reg_scaler.fit(X_train)\n",
    "\n",
    "spline_t = SplineTransformer(degree = 2, n_knots = 2)\n",
    "spline_t.fit(reg_scaler.transform(X_train))\n",
    "\n",
    "ridge_fit = RidgeCV(cv = 5)\n",
    "ridge_fit.fit(spline_t.transform(reg_scaler.transform(X_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = ridge_fit.predict(spline_t.transform(reg_scaler.transform(X_test)))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scaler = StandardScaler()\n",
    "reg_scaler.fit(X_train)\n",
    "\n",
    "rf_fit = RandomForestRegressor()\n",
    "rf_fit.fit(reg_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = rf_fit.predict(reg_scaler.transform(X_test))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth' : [int(x) for x in np.linspace(1, 20, 5)] + [None],\n",
    "    'n_estimators' : [int(x) for x in np.linspace(100, 2000, 10)]\n",
    "}\n",
    "rf_fit = RandomForestRegressor()\n",
    "clf_fit = GridSearchCV(rf_fit, param_grid)\n",
    "clf_fit.fit(reg_scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = clf_fit.predict(reg_scaler.transform(X_test))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = cat.CatBoostRegressor(iterations = 20000, depth = 8, od_type = \"Iter\", od_wait = 500)\n",
    "cat_fit.fit(reg_scaler.transform(X_train), y_train, eval_set = (reg_scaler.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reg = cat_fit.predict(reg_scaler.transform(X_test))\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, pred_reg))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
